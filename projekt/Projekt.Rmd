---
title: "Projekt Hitters - Oscar Teeninga i Tomasz Markiewicz"
output: html_document
---

```{r setup, include=FALSE}
install.packages("glmnet", repos = "https://cran.us.r-project.org")
install.packages("plotmo", repos = "https://cran.us.r-project.org")
library(ISLR)
library(class)
library(leaps)
library(MASS)
library(glmnet)
library(plotmo)
knitr::opts_chunk$set(message = FALSE)
```

# Zbiór danych
```{r dane}
print("Zbiór danych z 322 obserwacjami dotyczących główych graczy ligowych baseballu z roku 1986 i 1987. Liczba zmiennych: 20")
Hitters <- na.omit(Hitters)
print(Hitters)
```

# Podział zbioru na treningowy i testowy
```{r split dataset 2}
set.seed(123)
sample <- sample.int(n = nrow(Hitters), size = floor(.8*nrow(Hitters)), replace = F)
train <- Hitters[sample, ]
test  <- Hitters[-sample, ]
```

# Pierwszy problem regresji - League od pozostałych zmiennych (prócz NewLeague i Division)

## Cechy najbardziej znaczące
```{r league-cechy}
Hitters_league <- regsubsets(League ~ . - NewLeague - Division, data = Hitters, nvmax = 19)
plot(Hitters_league, scale="adjr2")
names(Hitters)
```
## Regresja logiczna
```{r league-glm}
legue_glm <- list()
legue_glm$fit <- glm(League ~ . - NewLeague - Division, family = binomial, data = train)
legue_glm$probs <- predict(legue_glm$fit, test, type = "response")
legue_glm$predicted <- ifelse(legue_glm$probs > 0.5, "N", "A")
table(legue_glm$predicted, test$League)
```

## LDA
```{r league-lda}
legue_lda <- list()
legue_lda$fit <- lda(League ~ . - NewLeague - Division, family = binomial, data = train)
legue_lda$predicted <- predict(legue_lda$fit, test)
table(legue_lda$predicted$class, test$League)
```

### Regresja wielomianowa N = 2 (Uwaga! Obliczenia długo trwają)
```{r league-lm2}
league_lm2 <- list()
league_lm2$fit <- lm(League ~ polym(AtBat,Hits,HmRun,Runs,RBI,Walks,Years,CAtBat,CHits,CHmRun,CRuns,CRBI,CWalks,PutOuts,Assists,Salary, degree = 2), data = train)
league_lm2$predicted <- predict(league_lm2$fit, test)
table(league_lm2$predicted, test$League)
# Nie widać wyraźnego miejsca w którym można byłoby postawić "kreskę", która oddzielałaby klasy między sobą, także wydaje się, że regresja wielomianowa przy tych parametrach nie daje fajnych efektów. 
```
## Regresja zmiennej 'New league'
### Regresje nieliniowe
## Regresje grzbietowe
```{r}
X <- model.matrix(League ~ . - NewLeague - Division, data = Hitters)[, -1]
y <- as.numeric(Hitters$NewLeague)
lambda.grid <- 10^seq(10, -2, length.out = 10)
fit.ridge <- glmnet(X, y, alpha = 0, family = "binomial")
set.seed(1)
n <- nrow(X)
train <- sample(1:n, n / 2)
test <- -train
```

dla przykładowego lambda = 7
```{r}
pred.ridge <- predict(fit.ridge, x = X[train,], y = y[train], s = 7, newx = X[test,])
mean((pred.ridge - y[test])^2)
```

dla optymalnego lambda
```{r}
cv.out <- cv.glmnet(X[train,], y[train], alpha = 0, family = "binomial")
plot(cv.out)
cv.out$lambda.min # optymalne lambda
fit.ridge <- glmnet(X[train,], y[train], alpha = 0, lambda = lambda.grid, thresh = 1e-12, family = "binomial")
pred.ridge <- predict(fit.ridge, s = cv.out$lambda.min, newx = X[test,])
mean((pred.ridge - y[test])^2)
```
## Regresja Lasso
```{r}
fit.lasso <- glmnet(X[train,], y[train], alpha = 1, family = "binomial")
cv.out <- cv.glmnet(X[train,], y[train], alpha = 1, family = "binomial")
plot(cv.out)
cv.out$lambda.min
pred.lasso <- predict(fit.lasso, s = cv.out$lambda.min, newx = X[test,])
mean((pred.lasso - y[test])^2)
```
## Wykresy regresja grzbietowa
```{r}
plot_glmnet(fit.ridge, labels =FALSE)
```
## Wykres regresji Lasso
```{r}
plot_glmnet(fit.lasso, labels =FALSE)
```


# Drugi problem regresji - Salary od reszty zmiennych
```{r salary-cechy}
Hitters_salary <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
plot(Hitters_salary, scale="adjr2")
```


# Podział zbioru na treningowy i testowy
```{r split dataset}
set.seed(123)
sample <- sample.int(n = nrow(Hitters), size = floor(.8*nrow(Hitters)), replace = F)
train <- Hitters[sample, ]
test  <- Hitters[-sample, ]
```

## Regrasja logiczna
```{r salary-glm}
salary_median = median(test$Salary)
salary_glm <- list()
salary_glm$fit <- glm(Salary ~ ., data = train)
salary_glm$probs <- predict(salary_glm$fit, test, type = "response")
salary_glm_salary_predicted <- ifelse(salary_glm$probs > salary_median, "Rich", "Poor")
salary_glm_salary = ifelse(test$Salary > salary_median, "Rich", "Poor")
table(salary_glm_salary_predicted, salary_glm_salary)
```


## LDA
```{r salary-lda}
salary_median = median(test$Salary)
salary_lda <- list()
salary_lda$fit <- lda(Salary ~ ., data = train)
salary_lda$predicted <- predict(salary_lda$fit, test)
salary_lda_salary_predicted = ifelse(as.numeric(salary_lda$predicted$class) > salary_median, "Rich", "Poor")
salary_lda_salary = ifelse(test$Salary > salary_median, "Rich", "Poor")
table(salary_lda_salary_predicted, salary_lda_salary)
```

## Regresja wielomianowa N = 2 (Uwaga! Obliczenia długo trwają, dlatego jest mniej zmiennych)
```{r salary-lm2}
salary_median = median(test$Salary)
salary_lm2 <- list()
salary_lm2$fit <- lm(Salary ~ polym(AtBat,Hits,Runs,RBI,Walks,Assists, degree = 2), data = train)
salary_lm2$predicted <- predict(salary_lm2$fit, test)
```


```{r salary-lm2 part 2}
salary_lm2_salary_predicted = ifelse(as.numeric(salary_lm2$predicted) > salary_median, "Rich", "Poor")
salary_lm2_salary = ifelse(test$Salary > salary_median, "Rich", "Poor")
table(salary_lm2_salary_predicted, salary_lm2_salary)
```

## Regresja zmiennej 'salary'
### Regresje liniowe
```{r}
cols <- names(Hitters)
cols <- cols[!cols %in% c("Salary")]
lm.hitters <- vector("list", length(cols))
for(i in seq_along(cols)){
    lm.hitters[[i]] <- lm(reformulate(cols[i], "Salary"), data = Hitters)
}
summaries <- lapply(lm.hitters, summary)
cols
summaries
```
### Regresje nieliniowe
## Regresje grzbietowe
```{r}
X <- model.matrix(Hitters$Salary ~ ., data = Hitters)[, -1]
y <- Hitters$Salary
fit.ridge <- glmnet(X, y, alpha = 0)
set.seed(1)
n <- nrow(X)
train <- sample(1:n, n / 2)
test <- -train
```

dla przykładowego lambda = 5
```{r}
pred.ridge <- predict(fit.ridge, x = X[train,], y = y[train], s = 7, newx = X[test,])
mean((pred.ridge - y[test])^2)
```

dla optymalnego lambda
```{r}
cv.out <- cv.glmnet(X[train,], y[train], alpha = 0)
plot(cv.out)
cv.out$lambda.min # optymalne lambda
fit.ridge <- glmnet(X[train,], y[train], alpha = 0, thresh = 1e-12)
pred.ridge <- predict(fit.ridge, s = cv.out$lambda.min, newx = X[test,])
mean((pred.ridge - y[test])^2)
```
## Regresja Lasso
```{r}
fit.lasso <- glmnet(X[train,], y[train], alpha = 1)
cv.out <- cv.glmnet(X[train,], y[train], alpha = 1)
plot(cv.out)
cv.out$lambda.min
pred.lasso <- predict(fit.lasso, s = cv.out$lambda.min, newx = X[test,])
mean((pred.lasso - y[test])^2)
```
## Wykresy regresja grzbietowa
```{r}
plot_glmnet(fit.ridge, labels =FALSE)
```
## Wykres regresji Lasso
```{r}
plot_glmnet(fit.lasso, labels =FALSE)
```
